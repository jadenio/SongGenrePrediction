{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyric_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otVOXVooY_y8"
      },
      "source": [
        "# Song Genre Classifier (via Lyric Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UYTyBf1uitX"
      },
      "source": [
        "**By Josiah Nielsen**\n",
        "\n",
        "**For: Collecting and Analyzing Big Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2BJTnakGvsa"
      },
      "source": [
        "This notebook contains code to analyze a dataset of song lyrics and genres. I will attempt to classify the genre of each song using only the song lyrics, through the use of various algorithms and feature extraction.  \n",
        "\n",
        "First, I will preprocess the data. This consists of removing punctuation and stop words from the lyrics. The lyrics are then tokenized and stemmed.\n",
        "\n",
        "Then, I performed classification (Naive Bayes, SVM, XGBoost, RandomForests, etc), using Count and TF-IDF Vectorization before classification. \n",
        "\n",
        "Finally, I performed feature extraction using Parts of Speech and Embeddding (Word2Vec), followed by classification modelling using these features. These models performed poorly, and only my Gradient Boosting Classifer with Word2Vec has been retained to demonstrate use of other methods.\n",
        "\n",
        "The XGBoost classifier w/ TF-IDF vectorization performed the best out of all models, with a 63% accuracy. The SVM classifier performed almost as good, with an accuracy of 62%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBvgZvpCZBOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc342ec-76ec-4b2e-bad5-e64541c074b3"
      },
      "source": [
        "#Import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm4aMuk6QILF"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v70Ey7gYgl9"
      },
      "source": [
        "#Load Data\n",
        "df = pd.read_csv(\"SongLyrics.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDupDwd8kN5d"
      },
      "source": [
        "#Remove songs with too few lyrics\n",
        "df['word_count'] = df['lyrics'].str.split( ).str.len()\n",
        "df = df[df['word_count'] > 50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GojpfYnykv2b",
        "outputId": "bf882cc7-4974-469c-ab62-f90a825f73e3"
      },
      "source": [
        "df.genre.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Country    12339\n",
              "Pop        12070\n",
              "Rock       11875\n",
              "Metal      11346\n",
              "Hip-Hop    10651\n",
              "Name: genre, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF4I7GtemrSS"
      },
      "source": [
        "#Create copy of unprocessed dataframe\n",
        "df_unprocessed = df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-9__mDjl82u"
      },
      "source": [
        "#Define function for preprocessing the lyrics\n",
        "def preprocessText(text, remove_stops=True):\n",
        "    \n",
        "    # Remove everything between hard brackets\n",
        "    text = re.sub(pattern=\"\\[.+?\\]( )?\", repl='', string=text)\n",
        "\n",
        "    # Change \"walkin'\" to \"walking\", for example\n",
        "    text = re.sub(pattern=\"n\\\\\\' \", repl='ng ', string=text)\n",
        "\n",
        "    # Remove x4 and (x4), for example\n",
        "    text = re.sub(pattern=\"(\\()?x\\d+(\\))?\", repl=' ', string=text)\n",
        "\n",
        "    # Fix apostrophe issues\n",
        "    text = re.sub(pattern=\"\\\\x91\", repl=\"'\", string=text)\n",
        "    text = re.sub(pattern=\"\\\\x92\", repl=\"'\", string=text)\n",
        "    text = re.sub(pattern=\"<u\\+0092>\", repl=\"'\", string=text)\n",
        "    \n",
        "    # Make lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Special cases/words\n",
        "    text = re.sub(pattern=\"'til\", repl=\"til\", string=text)\n",
        "    text = re.sub(pattern=\"'til\", repl=\"til\", string=text)\n",
        "    text = re.sub(pattern=\"gon'\", repl=\"gon\", string=text)\n",
        "\n",
        "    # Remove \\n from beginning\n",
        "    text = re.sub(pattern='^\\n', repl='', string=text)\n",
        "\n",
        "    # Strip , ! ?, : and remaining \\n from lyrics\n",
        "    text = ''.join([char.strip(\",!?:\") for char in text])\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    # Remove contractions\n",
        "    # specific\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"won\\’t\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"can\\’t\", \"can not\", text)\n",
        "    text = re.sub(r\"let's\", \"let us\", text)\n",
        "    text = re.sub(r\"let\\’s\", \"let us\", text)\n",
        "    text = re.sub(r\"ain't\", \"aint\", text)\n",
        "    text = re.sub(r\"ain\\’t\", \"aint\", text)\n",
        "\n",
        "     # general\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"n\\’t\", \" not\", text)\n",
        "    text = re.sub(r\"\\’re\", \" are\", text)\n",
        "    text = re.sub(r\"\\’s\", \" is\", text)\n",
        "    text = re.sub(r\"\\’d\", \" would\", text)\n",
        "    text = re.sub(r\"\\’ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\’t\", \" not\", text)\n",
        "    text = re.sub(r\"\\’ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\’m\", \" am\", text)\n",
        "\n",
        "    # Remove remaining punctuation\n",
        "    #punc = string.punctuation\n",
        "   # text = ''.join([char for char in text if char not in punc])\n",
        "\n",
        "    # Remove stopwords\n",
        "    if remove_stops:\n",
        "        stops = stopwords.words('english')\n",
        "        text = ' '.join([word for word in text.split(' ') if word not in stops])\n",
        "    \n",
        "    # Remove double spaces and beginning/trailing whitespace\n",
        "    text = re.sub(pattern='( ){2,}', repl=' ', string=text)\n",
        "    text = text.strip()\n",
        "    \n",
        "    return(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7I1x_cTmZAl"
      },
      "source": [
        "#Apply preprocessing function to data\n",
        "df['clean_lyrics'] = df.apply(lambda x: preprocessText(x['lyrics']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_LgZMDlCndd"
      },
      "source": [
        "#Save processed dataframe to .csv\n",
        "df.to_csv(\"lyrics_clean.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ler0ZQ93aHh_"
      },
      "source": [
        "#Function for stemming the lyrics\n",
        "df['stemmed_lyrics'] = df.apply(lambda x: stemming(x['lyrics']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEJEuu5DvnMH"
      },
      "source": [
        "Tokenization was not needed for the models below, as the CountVectorizer and TfidfVectorizer both tokenize the text on their own. Stemming was still performed as it improves model fit. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLlEskYCscnS"
      },
      "source": [
        "# Classification Models (with CountVectorizer and TfidfVectorizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w4R6QGStOFU"
      },
      "source": [
        "**Support Vector Machine (SVM)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb9z6aeYelKv"
      },
      "source": [
        "I trained an SVM model using SGDClassifier and TfidfVectorizer. The SVM model performed the second best of all models attempted, with slightly lower accuracy than the XGBoost model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqY4eCG4tft_",
        "outputId": "d5932106-2a34-4050-dd4f-712addd47835"
      },
      "source": [
        "#SVM model with k-fold CV\n",
        "text_svm = Pipeline([('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
        "                     ('svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4,\n",
        "                                           max_iter=25, random_state=123))])\n",
        "text_svm = text_svm.fit(train.stemmed_lyrics, train.genre)\n",
        "cross_val_score(estimator=text_svm, X=train.stemmed_lyrics, y=train.genre, cv=7).mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6219237180115698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "_KwTcRpgtx8K",
        "outputId": "9c13bccf-f011-416c-f01f-a2057d7ca415"
      },
      "source": [
        "#SVM test results\n",
        "print(text_svm.score(y=test.genre, X=test.stemmed_lyrics))\n",
        "preds_svm = text_svm.predict(test.stemmed_lyrics)\n",
        "print(classification_report(y_pred=preds_svm, y_true=test.genre))\n",
        "pd.crosstab(preds_svm, test.genre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6215041464112097\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Country       0.58      0.86      0.69      3702\n",
            "     Hip-Hop       0.73      0.86      0.79      3195\n",
            "       Metal       0.64      0.84      0.73      3404\n",
            "         Pop       0.56      0.42      0.48      3621\n",
            "        Rock       0.54      0.16      0.25      3563\n",
            "\n",
            "    accuracy                           0.62     17485\n",
            "   macro avg       0.61      0.63      0.59     17485\n",
            "weighted avg       0.61      0.62      0.58     17485\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>genre</th>\n",
              "      <th>Country</th>\n",
              "      <th>Hip-Hop</th>\n",
              "      <th>Metal</th>\n",
              "      <th>Pop</th>\n",
              "      <th>Rock</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Country</th>\n",
              "      <td>3179</td>\n",
              "      <td>105</td>\n",
              "      <td>134</td>\n",
              "      <td>941</td>\n",
              "      <td>1151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hip-Hop</th>\n",
              "      <td>67</td>\n",
              "      <td>2746</td>\n",
              "      <td>151</td>\n",
              "      <td>467</td>\n",
              "      <td>313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metal</th>\n",
              "      <td>177</td>\n",
              "      <td>92</td>\n",
              "      <td>2855</td>\n",
              "      <td>415</td>\n",
              "      <td>920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pop</th>\n",
              "      <td>197</td>\n",
              "      <td>219</td>\n",
              "      <td>165</td>\n",
              "      <td>1521</td>\n",
              "      <td>613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rock</th>\n",
              "      <td>82</td>\n",
              "      <td>33</td>\n",
              "      <td>99</td>\n",
              "      <td>277</td>\n",
              "      <td>566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "genre    Country  Hip-Hop  Metal   Pop  Rock\n",
              "row_0                                       \n",
              "Country     3179      105    134   941  1151\n",
              "Hip-Hop       67     2746    151   467   313\n",
              "Metal        177       92   2855   415   920\n",
              "Pop          197      219    165  1521   613\n",
              "Rock          82       33     99   277   566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yccKPEF0pMut"
      },
      "source": [
        "The SVM model heavily misclassified Country as Rock and Pop, similarly to the Naive Bayes model. The lyrics in these genres can tend to be similar, so it is not too surprising. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj64VaKnt3uX"
      },
      "source": [
        "#Which songs did the SVM model misclassify?\n",
        "test['preds_svm'] = preds_svm\n",
        "misclasses_svm = test[test.preds_svm != test.genre]\n",
        "misclasses_svm['misclass_combo'] = misclasses_svm.apply(lambda x: x['genre']+'-'+x['preds_svm'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr8ST-VQt8ZZ",
        "outputId": "23e0247e-5022-49b8-bde6-776a99fb23b3"
      },
      "source": [
        "#Number of misclasses per pair of genres. \n",
        "misclasses_svm.misclass_combo.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rock-Country       1151\n",
              "Pop-Country         941\n",
              "Rock-Metal          920\n",
              "Rock-Pop            613\n",
              "Pop-Hip-Hop         467\n",
              "Pop-Metal           415\n",
              "Rock-Hip-Hop        313\n",
              "Pop-Rock            277\n",
              "Hip-Hop-Pop         219\n",
              "Country-Pop         197\n",
              "Country-Metal       177\n",
              "Metal-Pop           165\n",
              "Metal-Hip-Hop       151\n",
              "Metal-Country       134\n",
              "Hip-Hop-Country     105\n",
              "Metal-Rock           99\n",
              "Hip-Hop-Metal        92\n",
              "Country-Rock         82\n",
              "Country-Hip-Hop      67\n",
              "Hip-Hop-Rock         33\n",
              "Name: misclass_combo, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA7l2sq3uZwT"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KFtK3s3dz-"
      },
      "source": [
        "Now I fit an XGBoost classifier model, in conjunction with the TfidfVectorizer. This model performed the best of all the models/parameterizations I attempted, with an accuracy of 63.3%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQWHq_vUucak"
      },
      "source": [
        "#XGB model\n",
        "vect = TfidfVectorizer(ngram_range=(1,2))\n",
        "vect.fit_transform(train.stemmed_lyrics)\n",
        "vect_train = vect.transform(pd.Series(train.stemmed_lyrics))\n",
        "vect_test = vect.transform(pd.Series(test.stemmed_lyrics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsmqmay0udDT"
      },
      "source": [
        "#Define XGBoost model\n",
        "xgb = XGBClassifier(learning_rate=0.25, subsample=0.8, gamma=1, random_state=123, max_depth=4, max_delta_step=1).fit(vect_train, train.genre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "5FPzhLH7umEq",
        "outputId": "418afe6a-6496-4f5e-ef0d-f50858df8397"
      },
      "source": [
        "#Look at results of XGBoost model\n",
        "print(xgb.score(y=test.genre, X=vect_test))\n",
        "preds_xgb = xgb.predict(vect_test)\n",
        "print(classification_report(y_pred=preds_xgb, y_true=test.genre))\n",
        "pd.crosstab(xgb.predict(vect_test), test.genre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6327137546468401\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Country       0.64      0.73      0.68      3702\n",
            "     Hip-Hop       0.88      0.79      0.83      3195\n",
            "       Metal       0.70      0.73      0.72      3404\n",
            "         Pop       0.54      0.51      0.52      3621\n",
            "        Rock       0.43      0.42      0.43      3563\n",
            "\n",
            "    accuracy                           0.63     17485\n",
            "   macro avg       0.64      0.64      0.64     17485\n",
            "weighted avg       0.63      0.63      0.63     17485\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>genre</th>\n",
              "      <th>Country</th>\n",
              "      <th>Hip-Hop</th>\n",
              "      <th>Metal</th>\n",
              "      <th>Pop</th>\n",
              "      <th>Rock</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Country</th>\n",
              "      <td>2699</td>\n",
              "      <td>70</td>\n",
              "      <td>116</td>\n",
              "      <td>547</td>\n",
              "      <td>753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hip-Hop</th>\n",
              "      <td>16</td>\n",
              "      <td>2527</td>\n",
              "      <td>65</td>\n",
              "      <td>203</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metal</th>\n",
              "      <td>129</td>\n",
              "      <td>85</td>\n",
              "      <td>2495</td>\n",
              "      <td>235</td>\n",
              "      <td>599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Pop</th>\n",
              "      <td>371</td>\n",
              "      <td>344</td>\n",
              "      <td>220</td>\n",
              "      <td>1839</td>\n",
              "      <td>648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rock</th>\n",
              "      <td>487</td>\n",
              "      <td>169</td>\n",
              "      <td>508</td>\n",
              "      <td>797</td>\n",
              "      <td>1503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "genre    Country  Hip-Hop  Metal   Pop  Rock\n",
              "row_0                                       \n",
              "Country     2699       70    116   547   753\n",
              "Hip-Hop       16     2527     65   203    60\n",
              "Metal        129       85   2495   235   599\n",
              "Pop          371      344    220  1839   648\n",
              "Rock         487      169    508   797  1503"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNp6Hqya1aPX"
      },
      "source": [
        "The XGBoost model performed the best of all the models I attempted. It classified songs in the Hip-Hop (f1=0.83) and Metal (f1=0.72) genres the most accurately, while poorly classifying songs in the Rock genre (f1=0.43)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wt6ttg7ut-H"
      },
      "source": [
        "#Which songs did the xgb model misclassify?\n",
        "test['preds_xgb'] = preds_xgb\n",
        "misclasses_xgb = test[test.preds_xgb != test.genre]\n",
        "misclasses_xgb['misclass_combo'] = misclasses_xgb.apply(lambda x: x['genre']+'-'+x['preds_xgb'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2b7fOC02Vv3",
        "outputId": "de4d4c9f-a025-4c1c-b506-33acfcf6f153"
      },
      "source": [
        "#Misclassified songs for each genre pair\n",
        "misclasses_xgb.misclass_combo.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pop-Rock           797\n",
              "Rock-Country       753\n",
              "Rock-Pop           648\n",
              "Rock-Metal         599\n",
              "Pop-Country        547\n",
              "Metal-Rock         508\n",
              "Country-Rock       487\n",
              "Country-Pop        371\n",
              "Hip-Hop-Pop        344\n",
              "Pop-Metal          235\n",
              "Metal-Pop          220\n",
              "Pop-Hip-Hop        203\n",
              "Hip-Hop-Rock       169\n",
              "Country-Metal      129\n",
              "Metal-Country      116\n",
              "Hip-Hop-Metal       85\n",
              "Hip-Hop-Country     70\n",
              "Metal-Hip-Hop       65\n",
              "Rock-Hip-Hop        60\n",
              "Country-Hip-Hop     16\n",
              "Name: misclass_combo, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI9oA2HfQAeU"
      },
      "source": [
        "# Classification Using Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0QUFU27QEX3"
      },
      "source": [
        "**Word2Vec - Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_NAx7uCQ6m5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78544d61-b6a3-45fd-e5f0-b75c778ee8b9"
      },
      "source": [
        "#Import Dependencies \n",
        "import gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3oAfaXjf0Or",
        "outputId": "c509ff92-611f-481c-ad56-7fee481c96d6"
      },
      "source": [
        "#Tokenize lyrics using simple_preprocessor\n",
        "from gensim.utils import simple_preprocess\n",
        "# Tokenize the text column to get the new column 'tokenized_text'\n",
        "df['tokenized_lyrics'] = [simple_preprocess(line, deacc=True) for line in df['clean_lyrics']] \n",
        "print(df['tokenized_lyrics'].head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     [love, crawdads, wheels, old, mag, fact, joint...\n",
            "2     [female, newsreporter, talking, also, hearing,...\n",
            "3     [ala, derecha, dale, suave, cabra³n, dale, sua...\n",
            "4     [uh, ah, uh, uhh, hahaha, check, flipmode, squ...\n",
            "5     [verse, getting, hard, tell, months, weeks, st...\n",
            "6     [living, phat, pockets, flat, wit, tha, gat, r...\n",
            "7     [word, bond, got, goin, amon, throw, ya, hands...\n",
            "8     [gorilla, fucking, coupe, finna, pull, zoo, ni...\n",
            "10    [many, brothers, fell, victim, streets, rest, ...\n",
            "11    [ohhh, ohhh, ohhhhhh, ohh, ohhh, ohhhhh, heard...\n",
            "Name: tokenized_lyrics, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBSVsbF8fnv_"
      },
      "source": [
        "#Stem the tokenized lyrics via PorterStemmer\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['tokenized_lyrics'] ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUpebwpcREWC"
      },
      "source": [
        "#Create list of tokenized lyrics\n",
        "Lyrics=list(df['clean_lyrics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU3l0mImROPQ"
      },
      "source": [
        "#Create corpus from the list of lyrics \n",
        "corpus=(Lyrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iewOYGAKRPQS"
      },
      "source": [
        "#Tokenize and Create Vocabulary List using the lyrics and the very large Brown Corpus of words\n",
        "sentence_data=[] \n",
        "Tokens=[]\n",
        "for i in range(len(corpus)):\n",
        "    g=corpus[i].split()\n",
        "    sentence_data.append(set(g))\n",
        "    Tokens.append((g))\n",
        "#Brown Corpus Data\n",
        "sentence_brown = brown.sents()   \n",
        "for i in range(len(sentence_brown)):\n",
        "    sentence_data.append(set(sentence_brown[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h1NvKLHSIKV"
      },
      "source": [
        "#Fit lyrics data to gensim model\n",
        "model = gensim.models.Word2Vec(sentence_data, size=50,window=5,min_count=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOEvH2dZSKUS"
      },
      "source": [
        "#Create the vocab list from the fitted Word2Vec model\n",
        "vocab=model.wv.vocab\n",
        "vocab=(set(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlqmFWgDSs27"
      },
      "source": [
        "#Apply Word2Vec to all of the data\n",
        "word2vecTokens=[] \n",
        "i=0;\n",
        "#Run a loop over all the tokens\n",
        "for g in Tokens: \n",
        "    vc=[] #to store temp vector for each token in an instance\n",
        "    for s in g:\n",
        "        if (s in vocab):\n",
        "            vc.append(model.wv[s])\n",
        "    word2vecTokens.append(vc) # appending all the vectors of an instance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAa1YSojS4Aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a11523-4b77-441d-c04a-574c36daca2e"
      },
      "source": [
        "g=[]\n",
        "for i in range (len(word2vecTokens)):\n",
        "    g.append(np.sum(word2vecTokens[i], axis=0)/len(word2vecTokens[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFFT3l5kTEUy"
      },
      "source": [
        "#Create the columns and append our embedded tokens\n",
        "columns=[]\n",
        "index=[]\n",
        "for k in range(0,57096):\n",
        "    index.append(k)\n",
        "for i in range(1,51):\n",
        "    columns.append(\"w2v_\"+str(i))\n",
        "df_ = pd.DataFrame(columns=columns)\n",
        "df_ = df_.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04PNUrbTTGcX"
      },
      "source": [
        "for x in range (len(g)):\n",
        "    try:\n",
        "        if(type(g[x])==float):\n",
        "            g[x]=[0]*50\n",
        "        g[x]=g[x].tolist()\n",
        "    except:\n",
        "        print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5o_hNZPTI8_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "4a7ad587-4752-4264-efdf-ce06161380cc"
      },
      "source": [
        "#Convert the Word2Vec features into a pandas dataframe\n",
        "for x in range (len(g)):\n",
        "    try:\n",
        "        if(type(g[x])==float):\n",
        "            g[x]=[0]*50\n",
        "    except:\n",
        "        print(x)\n",
        "w2v_df=pd.DataFrame(g,columns=columns)\n",
        "w2v_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>w2v_1</th>\n",
              "      <th>w2v_2</th>\n",
              "      <th>w2v_3</th>\n",
              "      <th>w2v_4</th>\n",
              "      <th>w2v_5</th>\n",
              "      <th>w2v_6</th>\n",
              "      <th>w2v_7</th>\n",
              "      <th>w2v_8</th>\n",
              "      <th>w2v_9</th>\n",
              "      <th>w2v_10</th>\n",
              "      <th>w2v_11</th>\n",
              "      <th>w2v_12</th>\n",
              "      <th>w2v_13</th>\n",
              "      <th>w2v_14</th>\n",
              "      <th>w2v_15</th>\n",
              "      <th>w2v_16</th>\n",
              "      <th>w2v_17</th>\n",
              "      <th>w2v_18</th>\n",
              "      <th>w2v_19</th>\n",
              "      <th>w2v_20</th>\n",
              "      <th>w2v_21</th>\n",
              "      <th>w2v_22</th>\n",
              "      <th>w2v_23</th>\n",
              "      <th>w2v_24</th>\n",
              "      <th>w2v_25</th>\n",
              "      <th>w2v_26</th>\n",
              "      <th>w2v_27</th>\n",
              "      <th>w2v_28</th>\n",
              "      <th>w2v_29</th>\n",
              "      <th>w2v_30</th>\n",
              "      <th>w2v_31</th>\n",
              "      <th>w2v_32</th>\n",
              "      <th>w2v_33</th>\n",
              "      <th>w2v_34</th>\n",
              "      <th>w2v_35</th>\n",
              "      <th>w2v_36</th>\n",
              "      <th>w2v_37</th>\n",
              "      <th>w2v_38</th>\n",
              "      <th>w2v_39</th>\n",
              "      <th>w2v_40</th>\n",
              "      <th>w2v_41</th>\n",
              "      <th>w2v_42</th>\n",
              "      <th>w2v_43</th>\n",
              "      <th>w2v_44</th>\n",
              "      <th>w2v_45</th>\n",
              "      <th>w2v_46</th>\n",
              "      <th>w2v_47</th>\n",
              "      <th>w2v_48</th>\n",
              "      <th>w2v_49</th>\n",
              "      <th>w2v_50</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.209785</td>\n",
              "      <td>0.181589</td>\n",
              "      <td>0.110289</td>\n",
              "      <td>0.109864</td>\n",
              "      <td>0.298785</td>\n",
              "      <td>-0.378487</td>\n",
              "      <td>0.146127</td>\n",
              "      <td>0.044905</td>\n",
              "      <td>0.021040</td>\n",
              "      <td>0.100183</td>\n",
              "      <td>-0.000413</td>\n",
              "      <td>-0.820316</td>\n",
              "      <td>-0.157412</td>\n",
              "      <td>-0.409688</td>\n",
              "      <td>-0.852307</td>\n",
              "      <td>0.228529</td>\n",
              "      <td>-0.537322</td>\n",
              "      <td>0.429066</td>\n",
              "      <td>0.378882</td>\n",
              "      <td>-0.032191</td>\n",
              "      <td>0.105517</td>\n",
              "      <td>0.440574</td>\n",
              "      <td>0.349704</td>\n",
              "      <td>0.657562</td>\n",
              "      <td>-0.185920</td>\n",
              "      <td>-0.477514</td>\n",
              "      <td>0.345601</td>\n",
              "      <td>0.204147</td>\n",
              "      <td>-0.075273</td>\n",
              "      <td>-0.206848</td>\n",
              "      <td>-0.045877</td>\n",
              "      <td>-0.069673</td>\n",
              "      <td>-0.066996</td>\n",
              "      <td>-0.186448</td>\n",
              "      <td>-0.012974</td>\n",
              "      <td>0.323681</td>\n",
              "      <td>-0.281494</td>\n",
              "      <td>0.945937</td>\n",
              "      <td>-0.542090</td>\n",
              "      <td>0.494095</td>\n",
              "      <td>-0.157334</td>\n",
              "      <td>-0.785356</td>\n",
              "      <td>-0.271331</td>\n",
              "      <td>-0.439568</td>\n",
              "      <td>0.191536</td>\n",
              "      <td>0.002863</td>\n",
              "      <td>0.060437</td>\n",
              "      <td>0.313540</td>\n",
              "      <td>-1.144182</td>\n",
              "      <td>-0.270229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.147199</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.371582</td>\n",
              "      <td>0.243956</td>\n",
              "      <td>0.185480</td>\n",
              "      <td>-0.534881</td>\n",
              "      <td>-0.071013</td>\n",
              "      <td>0.204643</td>\n",
              "      <td>0.098194</td>\n",
              "      <td>0.150684</td>\n",
              "      <td>0.173831</td>\n",
              "      <td>-0.347603</td>\n",
              "      <td>-0.317625</td>\n",
              "      <td>-0.494478</td>\n",
              "      <td>-0.641134</td>\n",
              "      <td>0.077006</td>\n",
              "      <td>-0.380763</td>\n",
              "      <td>0.288521</td>\n",
              "      <td>0.307991</td>\n",
              "      <td>0.023034</td>\n",
              "      <td>-0.059503</td>\n",
              "      <td>-0.039249</td>\n",
              "      <td>0.250375</td>\n",
              "      <td>0.601551</td>\n",
              "      <td>-0.165686</td>\n",
              "      <td>-0.347886</td>\n",
              "      <td>0.210369</td>\n",
              "      <td>0.256628</td>\n",
              "      <td>-0.025670</td>\n",
              "      <td>-0.297950</td>\n",
              "      <td>-0.246597</td>\n",
              "      <td>-0.074416</td>\n",
              "      <td>0.073985</td>\n",
              "      <td>-0.604431</td>\n",
              "      <td>-0.149943</td>\n",
              "      <td>0.260114</td>\n",
              "      <td>-0.182868</td>\n",
              "      <td>0.621283</td>\n",
              "      <td>-0.378396</td>\n",
              "      <td>0.392131</td>\n",
              "      <td>-0.086574</td>\n",
              "      <td>-0.417222</td>\n",
              "      <td>-0.345181</td>\n",
              "      <td>-0.567082</td>\n",
              "      <td>0.284757</td>\n",
              "      <td>0.004774</td>\n",
              "      <td>0.019047</td>\n",
              "      <td>0.459570</td>\n",
              "      <td>-1.035808</td>\n",
              "      <td>-0.219823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.023518</td>\n",
              "      <td>-0.641457</td>\n",
              "      <td>-1.102616</td>\n",
              "      <td>0.380452</td>\n",
              "      <td>-1.563758</td>\n",
              "      <td>-0.624014</td>\n",
              "      <td>0.615991</td>\n",
              "      <td>1.620445</td>\n",
              "      <td>0.710752</td>\n",
              "      <td>0.300238</td>\n",
              "      <td>-0.407481</td>\n",
              "      <td>-0.011696</td>\n",
              "      <td>1.617881</td>\n",
              "      <td>0.648333</td>\n",
              "      <td>1.317425</td>\n",
              "      <td>0.245065</td>\n",
              "      <td>-1.071978</td>\n",
              "      <td>0.299933</td>\n",
              "      <td>0.573878</td>\n",
              "      <td>0.937800</td>\n",
              "      <td>-0.032214</td>\n",
              "      <td>-0.418710</td>\n",
              "      <td>-1.294968</td>\n",
              "      <td>-1.844275</td>\n",
              "      <td>-1.447322</td>\n",
              "      <td>0.665796</td>\n",
              "      <td>-2.583090</td>\n",
              "      <td>-0.276443</td>\n",
              "      <td>-0.919307</td>\n",
              "      <td>-1.212407</td>\n",
              "      <td>0.919041</td>\n",
              "      <td>-2.441631</td>\n",
              "      <td>0.402402</td>\n",
              "      <td>0.129662</td>\n",
              "      <td>1.938810</td>\n",
              "      <td>1.376162</td>\n",
              "      <td>-0.031456</td>\n",
              "      <td>-0.560316</td>\n",
              "      <td>-0.513700</td>\n",
              "      <td>0.740478</td>\n",
              "      <td>0.116013</td>\n",
              "      <td>0.450140</td>\n",
              "      <td>-0.342815</td>\n",
              "      <td>-0.540263</td>\n",
              "      <td>0.702031</td>\n",
              "      <td>-0.791063</td>\n",
              "      <td>1.578859</td>\n",
              "      <td>1.100754</td>\n",
              "      <td>-0.190145</td>\n",
              "      <td>1.338918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.419271</td>\n",
              "      <td>0.081302</td>\n",
              "      <td>0.228797</td>\n",
              "      <td>0.156991</td>\n",
              "      <td>-0.062284</td>\n",
              "      <td>-0.295888</td>\n",
              "      <td>-0.123680</td>\n",
              "      <td>-0.054129</td>\n",
              "      <td>-0.107155</td>\n",
              "      <td>0.173701</td>\n",
              "      <td>-0.043598</td>\n",
              "      <td>-0.361134</td>\n",
              "      <td>-0.134886</td>\n",
              "      <td>-0.504117</td>\n",
              "      <td>-0.730600</td>\n",
              "      <td>0.153167</td>\n",
              "      <td>-0.736951</td>\n",
              "      <td>0.262178</td>\n",
              "      <td>0.704675</td>\n",
              "      <td>-0.058566</td>\n",
              "      <td>-0.170795</td>\n",
              "      <td>0.531184</td>\n",
              "      <td>0.174029</td>\n",
              "      <td>0.765985</td>\n",
              "      <td>-0.321834</td>\n",
              "      <td>-0.084343</td>\n",
              "      <td>0.087420</td>\n",
              "      <td>0.368625</td>\n",
              "      <td>-0.315431</td>\n",
              "      <td>-0.255213</td>\n",
              "      <td>-0.395816</td>\n",
              "      <td>0.243046</td>\n",
              "      <td>0.134219</td>\n",
              "      <td>-0.186369</td>\n",
              "      <td>-0.047282</td>\n",
              "      <td>0.532313</td>\n",
              "      <td>-0.196392</td>\n",
              "      <td>1.082691</td>\n",
              "      <td>-0.249974</td>\n",
              "      <td>0.476764</td>\n",
              "      <td>-0.025872</td>\n",
              "      <td>-0.438880</td>\n",
              "      <td>-0.510833</td>\n",
              "      <td>-1.136053</td>\n",
              "      <td>-0.064628</td>\n",
              "      <td>-0.077814</td>\n",
              "      <td>-0.106368</td>\n",
              "      <td>0.475145</td>\n",
              "      <td>-1.341440</td>\n",
              "      <td>-0.276306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.489338</td>\n",
              "      <td>0.686841</td>\n",
              "      <td>-0.446464</td>\n",
              "      <td>0.351980</td>\n",
              "      <td>0.492617</td>\n",
              "      <td>-0.199488</td>\n",
              "      <td>0.246129</td>\n",
              "      <td>-0.238532</td>\n",
              "      <td>-0.061256</td>\n",
              "      <td>1.106874</td>\n",
              "      <td>-0.564325</td>\n",
              "      <td>-0.581480</td>\n",
              "      <td>-0.406552</td>\n",
              "      <td>-0.863759</td>\n",
              "      <td>-0.651030</td>\n",
              "      <td>0.327285</td>\n",
              "      <td>-0.983178</td>\n",
              "      <td>0.918216</td>\n",
              "      <td>0.010463</td>\n",
              "      <td>-0.175525</td>\n",
              "      <td>-0.296195</td>\n",
              "      <td>-0.080797</td>\n",
              "      <td>0.934673</td>\n",
              "      <td>1.084810</td>\n",
              "      <td>-0.220089</td>\n",
              "      <td>-1.163090</td>\n",
              "      <td>0.935209</td>\n",
              "      <td>0.449411</td>\n",
              "      <td>-0.035598</td>\n",
              "      <td>0.508712</td>\n",
              "      <td>-0.063932</td>\n",
              "      <td>0.081240</td>\n",
              "      <td>-0.268076</td>\n",
              "      <td>-0.354819</td>\n",
              "      <td>0.193150</td>\n",
              "      <td>-0.078406</td>\n",
              "      <td>0.109638</td>\n",
              "      <td>0.786496</td>\n",
              "      <td>0.520406</td>\n",
              "      <td>0.774771</td>\n",
              "      <td>0.619919</td>\n",
              "      <td>-1.411840</td>\n",
              "      <td>-0.000969</td>\n",
              "      <td>-0.530563</td>\n",
              "      <td>0.288686</td>\n",
              "      <td>1.004326</td>\n",
              "      <td>0.331517</td>\n",
              "      <td>1.322752</td>\n",
              "      <td>-1.471923</td>\n",
              "      <td>-0.246995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      w2v_1     w2v_2     w2v_3  ...    w2v_48    w2v_49    w2v_50\n",
              "0  0.209785  0.181589  0.110289  ...  0.313540 -1.144182 -0.270229\n",
              "1  0.147199  0.214286  0.371582  ...  0.459570 -1.035808 -0.219823\n",
              "2 -0.023518 -0.641457 -1.102616  ...  1.100754 -0.190145  1.338918\n",
              "3  0.419271  0.081302  0.228797  ...  0.475145 -1.341440 -0.276306\n",
              "4 -0.489338  0.686841 -0.446464  ...  1.322752 -1.471923 -0.246995\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARhfIzQmTTZU"
      },
      "source": [
        "#Save Word2Vec data to .csv file\n",
        "w2v_df.to_csv(\"w2vData.csv\",index=False) # Saving the dataframe to a file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IRtN4cp7oTD"
      },
      "source": [
        "**Gradient Boosting Classification Using Word2Vec Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvfzbDFVk9l0"
      },
      "source": [
        "#Import dependencies\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeM2atWykX8C"
      },
      "source": [
        "#Create column for genres in Word2Vec dataframe\n",
        "w2v_df['genre'] = df['genre']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Beh2UgVMlJta"
      },
      "source": [
        "#Drop all NAs\n",
        "w2v_df = w2v_df.dropna(how='any')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU2z_yf6kP6r"
      },
      "source": [
        "#Create train/test datasets\n",
        "train,test,train_y,test_y = train_test_split(w2v_df[w2v_df.columns.difference(['genre'])],w2v_df['genre'],train_size=0.67)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLeIut3lk3VQ",
        "outputId": "6f95250d-3bbf-43d9-be42-22d947bb299b"
      },
      "source": [
        "#Fit gradient boosting classification model\n",
        "gd = GradientBoostingClassifier(max_depth=20)\n",
        "gd.fit(train,train_y)\n",
        "pred = gd.predict(test)\n",
        "print (accuracy_score(test_y,pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4504994224366379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeqMiZgyphG0"
      },
      "source": [
        "The Gradient Boosting Classifier achieved an accuracy of 45%, which is far lower than achieved by the XGBoost and SVM model. This is likely due to the fact that song lyrics often don't have cohesive word structures like, say, a book would. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsj01PkFqjtk"
      },
      "source": [
        "Other models that I tried included: Naive Bayes, SVM, Random Forests, and XGBoost. However, these all has even worse accuracy than the Gradient Boosting model. "
      ]
    }
  ]
}
